# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qjqg3X6Zfmaw9acpSITnfKJITXD-zMT1
"""

# Commented out IPython magic to ensure Python compatibility.
# prompt: connect with google drive to a specific folder

from google.colab import drive
drive.mount('/content/drive')

# Get the folder ID from the URL of the desired folder in Google Drive.
# For example, if the URL is "https://drive.google.com/drive/folders/1234567890",
# then the folder ID is "1234567890".
folder_id = 'examenes'

# Create a path to the desired folder.
folder_path = f'/content/drive/MyDrive/{folder_id}'

# Change the current working directory to the desired folder.
# %cd {folder_path}

"""# importing dataframe"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import classification_report, accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import balanced_accuracy_score
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.ensemble import RandomForestClassifier
!pip install catboost
from catboost import CatBoostClassifier

df=pd.read_csv("hr_ibm.csv")

"""# EDA"""

df.dtypes

df=df.drop_duplicates()
df= df.dropna()
df.isna().sum()

len(df)

#This function is to remove outliers. However, by using it, the dataset length is reduced from 1470 to 699
#def remove_outliers(df, col):
#  Q1 = df[col].quantile(0.25)
#  Q3 = df[col].quantile(0.75)
#  IQR = Q3 - Q1
#  lower_bound = Q1 - 1.5 * IQR
#  upper_bound = Q3 + 1.5 * IQR
#  df_filtered = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
#  return df_filtered

#for col in df.select_dtypes(include=["int", "float"]):
#  df = remove_outliers(df, col)

"""# Pre-processing"""

encoder = LabelEncoder()
columns_to_encode=["Attrition", "BusinessTravel", "Department", "EducationField", "Gender", "JobRole", "MaritalStatus", "Over18", "OverTime"]
for column in columns_to_encode:
  encoder.fit(df[column])
  df[column] = encoder.transform(df[column])

scaler = MinMaxScaler()
df_normalized = scaler.fit_transform(df)
df_normalized = pd.DataFrame(df_normalized, columns=df.columns)

"""# Train-Test"""

df_normalized

df_features=df_normalized.drop(columns=["Attrition"], axis=1)

X_train, X_test, y_train, y_test = train_test_split(df_features, df_normalized['Attrition'], test_size=0.2)

"""# Model 1: Decision Tree"""

max_depths = [None, 5, 10, 20, 50, 100]

accuracies = []

for max_depth in max_depths:
    clf = DecisionTreeClassifier(random_state=42, max_depth=max_depth)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)

best_max_depth = max_depths[np.argmax(accuracies)]

dt = DecisionTreeClassifier(random_state=42, max_depth=best_max_depth)

dt.fit(X_train, y_train)

y_pred_dt = clf.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred_dt))
print("\nClassification Report:\n", classification_report(y_test, y_pred_dt))

"""# Model 2: Logistic Regression"""

lr = LogisticRegression(random_state=42)
lr.fit(X_train, y_train)


y_pred_lr = lr.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print("\nClassification Report:\n", classification_report(y_test, y_pred_lr))

"""# Model 3: Random Forest"""

# Initialize and train a Random Forest classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Make predictions
y_pred_rf = rf.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))

"""# Model 4: XGBoost"""

xgb_clf = xgb.XGBClassifier(objective='multi:softmax',
                            num_class=3,
                            missing=1,
                            early_stopping_rounds=10,
                            eval_metric=['merror','mlogloss'],
                            seed=42)
xgb_clf.fit(X_train,
            y_train,
            verbose=0, # set to 1 to see xgb training round intermediate results
            eval_set=[(X_train, y_train), (X_test, y_test)])


results = xgb_clf.evals_result()
epochs = 1
x_axis = range(0, epochs)

y_pred_xg = xgb_clf.predict(X_test)
print("Accuracy:", balanced_accuracy_score(y_test, y_pred_xg))
print("\nClassification Report:\n", classification_report(y_test, y_pred_xg))

"""# Model 5: Catboost"""

model = CatBoostClassifier(iterations=5000,
                           learning_rate=0.05,
                           depth=10,
                           loss_function='Logloss',
                           eval_metric='Accuracy',
                           l2_leaf_reg=3)

model.fit(X_train, y_train,
         eval_set=(X_test, y_test),
         verbose=500,
         plot=True)

y_pred_cb = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred_cb))
print("\nClassification Report:\n", classification_report(y_test, y_pred_cb))

"""# Conclusions

Considering the five developed models, Catboost is the one that better predict the outcomes. However, dataset is imbalanced. For this reason, one category is better predicted.
"""